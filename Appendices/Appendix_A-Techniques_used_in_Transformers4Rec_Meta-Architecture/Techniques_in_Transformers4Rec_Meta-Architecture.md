# Soft one-hot encoding

The soft one-hot encoding method was originally proposed to encode the duration ofan event [li2017time], but we propose its usage to encode all continuous features. It represents a scalar as a weightedsum over all embeddings for that feature. More formally, the scalar feature𝑛is projected onto a vector space as in𝑝𝑛=𝑛𝑊𝑛+𝑏𝑛, where𝑊𝑛∈R1×𝑃is the weight matrix,𝑏𝑛∈R𝑃is the bias vector, and𝑃is the number of desiredembeddings for the feature𝑛embedding table. Then, asoftmaxfunction is applied to the projection vector𝑝𝑛, as𝑠𝑛=softmax(𝑝𝑛). Finally, the probability distribution obtained from thesoftmaxis used to do a weighted sum over anembedding space,𝑔𝑛=𝑠𝑛𝐸𝑛, where𝐸 𝑛∈R 𝑃×𝐷 is the embedding matrix for feature n and D is its embedding size.


# Tying embeddings


The NLP community has proposed this technique to tie the input embeddings matrix with theoutput projection layer matrix [inan2016tying, press2017using]. The main motivation is that for language models, boththe input and the output of the models are words, so they should lie in the same vector space. Analogously recommendersystems models have item ids as input and output. Deep recommender system models are generally memory-bound,and most of the parameters are concentrated in large embedding tables cite[zhang2018training]. In such a scenario,tying embeddings results in a significant reduction of memory requirements by holding only one projection matrix foritem and output representations. Additionally, rare item embeddings can benefit more from the output layer updates ateach training step.

A reduction in the number of parameters isn’t the only benefit to weight tying though. Under the recommendersystems taxonomy, tying embeddings technique introduces a matrix factorization operation between the item embed-dings and the final representation of the user or session, as we demonstrate below. Formally, let𝑛be the number ofitems,𝑑the dimension of the item embeddings, and𝑈𝑛×𝑑be the item embedding matrix. Let a neural network witharbitrary layers that takes the input features, including the item embeddings, and outputs a vector of activationsℎ∈R𝑠.The output projection matrix𝑉∈R𝑠×𝑛then mapsℎto the logits𝑙∈R𝑛for all items, by computing𝑙=ℎ𝑉. In order totie the embeddings, we make𝑈=𝑉, so that the embedding matrix are shared and𝑙=ℎ𝑉=ℎ𝑈⊤. It is worth notingthat GRU4Rec used weight tying (referred as constrained embeddings) in their publicly available source code but didnot mention the technique or its benefits in their paper. In this work, we propose adding a bias𝑏∈R𝑛to the outputprojection layer, making𝑙=(ℎ𝑈⊤)+𝑏, gives the output an additional degree of freedom from the input embeddings.We found that the bias addition has slightly improved model accuracies, so we use this variation of tying embeddings.
