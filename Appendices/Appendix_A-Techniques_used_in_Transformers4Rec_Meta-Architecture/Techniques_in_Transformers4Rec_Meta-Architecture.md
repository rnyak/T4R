# Soft one-hot encoding

The soft one-hot encoding method was originally proposed to encode the duration ofan event [li2017time], but we propose its usage to encode all continuous features. It represents a scalar as a weightedsum over all embeddings for that feature. More formally, the scalar featureğ‘›is projected onto a vector space as inğ‘ğ‘›=ğ‘›ğ‘Šğ‘›+ğ‘ğ‘›, whereğ‘Šğ‘›âˆˆR1Ã—ğ‘ƒis the weight matrix,ğ‘ğ‘›âˆˆRğ‘ƒis the bias vector, andğ‘ƒis the number of desiredembeddings for the featureğ‘›embedding table. Then, asoftmaxfunction is applied to the projection vectorğ‘ğ‘›, asğ‘ ğ‘›=softmax(ğ‘ğ‘›). Finally, the probability distribution obtained from thesoftmaxis used to do a weighted sum over anembedding space,ğ‘”ğ‘›=ğ‘ ğ‘›ğ¸ğ‘›, whereğ¸ ğ‘›âˆˆR ğ‘ƒÃ—ğ· is the embedding matrix for feature n and D is its embedding size.


# Tying embeddings


The NLP community has proposed this technique to tie the input embeddings matrix with theoutput projection layer matrix [inan2016tying, press2017using]. The main motivation is that for language models, boththe input and the output of the models are words, so they should lie in the same vector space. Analogously recommendersystems models have item ids as input and output. Deep recommender system models are generally memory-bound,and most of the parameters are concentrated in large embedding tables cite[zhang2018training]. In such a scenario,tying embeddings results in a significant reduction of memory requirements by holding only one projection matrix foritem and output representations. Additionally, rare item embeddings can benefit more from the output layer updates ateach training step.

A reduction in the number of parameters isnâ€™t the only benefit to weight tying though. Under the recommendersystems taxonomy, tying embeddings technique introduces a matrix factorization operation between the item embed-dings and the final representation of the user or session, as we demonstrate below. Formally, letğ‘›be the number ofitems,ğ‘‘the dimension of the item embeddings, andğ‘ˆğ‘›Ã—ğ‘‘be the item embedding matrix. Let a neural network witharbitrary layers that takes the input features, including the item embeddings, and outputs a vector of activationsâ„âˆˆRğ‘ .The output projection matrixğ‘‰âˆˆRğ‘ Ã—ğ‘›then mapsâ„to the logitsğ‘™âˆˆRğ‘›for all items, by computingğ‘™=â„ğ‘‰. In order totie the embeddings, we makeğ‘ˆ=ğ‘‰, so that the embedding matrix are shared andğ‘™=â„ğ‘‰=â„ğ‘ˆâŠ¤. It is worth notingthat GRU4Rec used weight tying (referred as constrained embeddings) in their publicly available source code but didnot mention the technique or its benefits in their paper. In this work, we propose adding a biasğ‘âˆˆRğ‘›to the outputprojection layer, makingğ‘™=(â„ğ‘ˆâŠ¤)+ğ‘, gives the output an additional degree of freedom from the input embeddings.We found that the bias addition has slightly improved model accuracies, so we use this variation of tying embeddings.
